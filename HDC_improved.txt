import numpy as np
from collections import defaultdict
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

class HDCAssembleImproved:
    
    @staticmethod
    def gerar_vetor_binario(dimension=10000, seed=None):
        """Gera vetor binário com seed para reprodutibilidade"""
        if seed is not None:
            np.random.seed(seed)
        return np.random.choice([-1, 1], size=dimension)
    
    @staticmethod
    def binding(v1, v2):
        """Operação de binding (XOR para vetores binários)"""
        return v1 * v2
    
    @staticmethod
    def bundling(vetores, weights=None):
        """Bundling melhorado com pesos opcionais"""
        if weights is not None:
            soma = np.average(vetores, axis=0, weights=weights)
        else:
            soma = np.mean(vetores, axis=0)
        return np.where(soma >= 0, 1.0, -1.0)
    
    @staticmethod
    def similaridade(v1, v2):
        """Múltiplas métricas de similaridade"""
        # Similaridade cosseno
        cosine = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        # Hamming normalizada
        hamming = 1 - np.sum(v1 != v2) / len(v1)
        return cosine
    
    @staticmethod
    def codificacao_termometro_melhorada(valor, n_niveis, tipo='linear'):
        """Codificação termômetro com diferentes tipos"""
        if tipo == 'linear':
            limite_superior = n_niveis - 1
            nivel_ativo = int(np.clip(valor * limite_superior, 0, limite_superior))
        elif tipo == 'logaritmica':
            if valor <= 0:
                nivel_ativo = 0
            else:
                nivel_ativo = int(np.clip(np.log10(valor * 9 + 1) * (n_niveis - 1), 0, n_niveis - 1))
        else:  # exponencial
            nivel_ativo = int(np.clip((valor ** 2) * (n_niveis - 1), 0, n_niveis - 1))
        
        vetor = np.full(n_niveis, -1)
        vetor[:nivel_ativo + 1] = 1
        return vetor

class HDCClassificadorMelhorado:
    def __init__(self, d_dimensao=10000, n_niveis=10, modo='record', epocas=5, 
                 taxa_aprendizado=0.1, regularizacao=0.01, dropout_rate=0.1,
                 ensemble_size=1, adaptive_learning=True, early_stopping=True,
                 patience=3, validation_split=0.2, seed=42):
        
        self.DIMENSION = d_dimensao
        self.n_niveis = n_niveis
        self.modo = modo
        self.epocas = epocas
        self.taxa_aprendizado = taxa_aprendizado
        self.regularizacao = regularizacao
        self.dropout_rate = dropout_rate
        self.ensemble_size = ensemble_size
        self.adaptive_learning = adaptive_learning
        self.early_stopping = early_stopping
        self.patience = patience
        self.validation_split = validation_split
        self.seed = seed
        
        # Inicialização com seeds para reprodutibilidade
        np.random.seed(self.seed)
        
        self.vetores_atributos = {}
        self.vetores_posicoes = {}
        self.prototipos_classes = {}
        self.ensemble_models = []
        self.assemble = HDCAssembleImproved()
        
        # Variáveis para controle de overfitting
        self.historico_loss = []
        self.historico_val_loss = []
        self.melhor_modelo = None
        self.contador_patience = 0
        
        # Normalização adaptativa
        self.scaler = StandardScaler()
        
    def _aplicar_dropout(self, vetor):
        """Aplica dropout para regularização"""
        if self.dropout_rate > 0:
            mask = np.random.binomial(1, 1 - self.dropout_rate, size=len(vetor))
            return vetor * mask
        return vetor
    
    def _regularizacao_l2(self, vetor):
        """Aplica regularização L2"""
        if self.regularizacao > 0:
            norma = np.linalg.norm(vetor)
            if norma > 0:
                return vetor * (1 - self.regularizacao * norma)
        return vetor
    
    def _codificar_exemplo_melhorado(self, exemplo, usar_dropout=False):
        """Codificação melhorada com múltiplas técnicas"""
        vetores = []
        
        # Normalização local
        exemplo_norm = (exemplo - np.mean(exemplo)) / (np.std(exemplo) + 1e-8)
        
        if self.modo == 'record':
            for i, valor in enumerate(exemplo_norm):
                # Inicializar vetores se necessário
                if i not in self.vetores_posicoes:
                    self.vetores_posicoes[i] = self.assemble.gerar_vetor_binario(
                        self.DIMENSION, seed=self.seed + i)
                
                vetor_posicao = self.vetores_posicoes[i]
                
                # Gerar vetores para cada nível
                for nivel in range(self.n_niveis):
                    chave = (i, nivel)
                    if chave not in self.vetores_atributos:
                        self.vetores_atributos[chave] = self.assemble.gerar_vetor_binario(
                            self.DIMENSION, seed=self.seed + hash(chave))
                
                # Codificação termômetro melhorada
                vetor_termo = self.assemble.codificacao_termometro_melhorada(
                    valor, self.n_niveis, tipo='linear')
                
                vetores_nivel = []
                for nivel in range(self.n_niveis):
                    if vetor_termo[nivel] == 1:
                        vetor_nivel = self.vetores_atributos[(i, nivel)]
                        vetor_bind = self.assemble.binding(vetor_nivel, vetor_posicao)
                        vetores_nivel.append(vetor_bind)
                
                if vetores_nivel:
                    vetor_atributo = self.assemble.bundling(vetores_nivel)
                    
                    # Aplicar dropout se especificado
                    if usar_dropout:
                        vetor_atributo = self._aplicar_dropout(vetor_atributo)
                    
                    vetores.append(vetor_atributo)
        
        elif self.modo == 'hierarchical':
            # Modo hierárquico - nova funcionalidade
            for i in range(len(exemplo_norm)):
                for j in range(i + 1, min(i + 3, len(exemplo_norm))):  # Janelas de tamanho 2-3
                    valores = exemplo_norm[i:j+1]
                    vetor_hierarquico = self._codificar_hierarquico(valores, i)
                    if usar_dropout:
                        vetor_hierarquico = self._aplicar_dropout(vetor_hierarquico)
                    vetores.append(vetor_hierarquico)
        
        # Bundling final com pesos adaptativos
        if len(vetores) > 1:
            # Calcular pesos baseados na variância
            variancias = [np.var(v) for v in vetores]
            pesos = np.array(variancias) / np.sum(variancias) if np.sum(variancias) > 0 else None
            vetor_final = self.assemble.bundling(vetores, weights=pesos)
        else:
            vetor_final = vetores[0] if vetores else np.ones(self.DIMENSION)
        
        # Aplicar regularização
        return self._regularizacao_l2(vetor_final)
    
    def _codificar_hierarquico(self, valores, posicao_inicial):
        """Codificação hierárquica para capturar padrões locais"""
        vetores_locais = []
        
        for i, valor in enumerate(valores):
            pos = posicao_inicial + i
            if pos not in self.vetores_posicoes:
                self.vetores_posicoes[pos] = self.assemble.gerar_vetor_binario(
                    self.DIMENSION, seed=self.seed + pos)
            
            vetor_termo = self.assemble.codificacao_termometro_melhorada(
                valor, self.n_niveis, tipo='linear')
            
            vetores_nivel = []
            for nivel in range(self.n_niveis):
                if vetor_termo[nivel] == 1:
                    chave = (pos, nivel)
                    if chave not in self.vetores_atributos:
                        self.vetores_atributos[chave] = self.assemble.gerar_vetor_binario(
                            self.DIMENSION, seed=self.seed + hash(chave))
                    
                    vetor_bind = self.assemble.binding(
                        self.vetores_atributos[chave], self.vetores_posicoes[pos])
                    vetores_nivel.append(vetor_bind)
            
            if vetores_nivel:
                vetores_locais.append(self.assemble.bundling(vetores_nivel))
        
        return self.assemble.bundling(vetores_locais)
    
    def _validacao_cruzada_interna(self, X, y):
        """Validação cruzada para evitar overfitting"""
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        
        split_idx = int(len(X) * (1 - self.validation_split))
        train_idx, val_idx = indices[:split_idx], indices[split_idx:]
        
        return X[train_idx], X[val_idx], y[train_idx], y[val_idx]
    
    def _calcular_loss(self, X, y, usar_dropout=False):
        """Calcula loss para monitoramento"""
        predictions = self.predict(X, usar_dropout=usar_dropout)
        accuracy = accuracy_score(y, predictions)
        
        # Loss baseado em distância dos protótipos
        loss = 0
        for exemplo, classe_real in zip(X, y):
            vetor = self._codificar_exemplo_melhorado(exemplo, usar_dropout=usar_dropout)
            if classe_real in self.prototipos_classes:
                dist = 1 - self.assemble.similaridade(vetor, self.prototipos_classes[classe_real])
                loss += dist
        
        return loss / len(X), accuracy
    
    def train(self, X, y):
        """Treinamento melhorado com técnicas anti-overfitting"""
        print(f"Iniciando treinamento melhorado com {len(X)} amostras...")
        
        # Normalização dos dados
        X_norm = self.scaler.fit_transform(X)
        
        # Divisão para validação se necessário
        if self.early_stopping:
            X_train, X_val, y_train, y_val = self._validacao_cruzada_interna(X_norm, y)
        else:
            X_train, y_train = X_norm, y
            X_val, y_val = None, None
        
        # Ensemble training
        for ensemble_idx in range(self.ensemble_size):
            print(f"Treinando modelo {ensemble_idx + 1}/{self.ensemble_size}")
            
            # Criar modelo individual
            modelo_individual = self._treinar_modelo_individual(X_train, y_train, X_val, y_val)
            self.ensemble_models.append(modelo_individual)
        
        # Se apenas um modelo, usar diretamente
        if self.ensemble_size == 1:
            self.prototipos_classes = self.ensemble_models[0]
    
    def _treinar_modelo_individual(self, X_train, y_train, X_val=None, y_val=None):
        """Treina um modelo individual do ensemble"""
        prototipos_locais = {}
        melhor_loss = float('inf')
        contador_patience = 0
        
        for epoca in range(self.epocas):
            print(f"  Época {epoca + 1}/{self.epocas}")
            
            # Embaralhar dados
            indices = np.arange(len(X_train))
            np.random.shuffle(indices)
            X_shuffled = X_train[indices]
            y_shuffled = y_train[indices]
            
            # Primeira época: inicialização
            if epoca == 0:
                prototipos_temp = defaultdict(list)
                for exemplo, classe in zip(X_shuffled, y_shuffled):
                    vetor = self._codificar_exemplo_melhorado(exemplo, usar_dropout=True)
                    prototipos_temp[classe].append(vetor)
                
                prototipos_locais = {
                    classe: self.assemble.bundling(vetores) 
                    for classe, vetores in prototipos_temp.items()
                }
            else:
                # Épocas subsequentes: refinamento
                for exemplo, classe in zip(X_shuffled, y_shuffled):
                    vetor = self._codificar_exemplo_melhorado(exemplo, usar_dropout=True)
                    
                    # Predição atual
                    melhor_classe = None
                    maior_sim = -np.inf
                    
                    for c, prototipo in prototipos_locais.items():
                        sim = self.assemble.similaridade(vetor, prototipo)
                        if sim > maior_sim:
                            maior_sim = sim
                            melhor_classe = c
                    
                    # Atualização adaptativa
                    if melhor_classe != classe:
                        # Taxa de aprendizado adaptativa
                        taxa_atual = self.taxa_aprendizado
                        if self.adaptive_learning:
                            taxa_atual *= (1 - epoca / self.epocas)  # Decaimento linear
                        
                        # Atualizar protótipo correto (reforço)
                        prototipos_locais[classe] = prototipos_locais[classe].astype(float)
                        prototipos_locais[classe] += taxa_atual * vetor
                        prototipos_locais[classe] = np.where(
                            prototipos_locais[classe] >= 0, 1.0, -1.0)
                        
                        # Penalizar protótipo errado
                        if melhor_classe in prototipos_locais:
                            prototipos_locais[melhor_classe] = prototipos_locais[melhor_classe].astype(float)
                            prototipos_locais[melhor_classe] -= taxa_atual * 0.5 * vetor
                            prototipos_locais[melhor_classe] = np.where(
                                prototipos_locais[melhor_classe] >= 0, 1.0, -1.0)
            
            # Early stopping
            if self.early_stopping and X_val is not None:
                # Temporariamente usar prototipos locais para validação
                prototipos_backup = self.prototipos_classes.copy()
                self.prototipos_classes = prototipos_locais
                
                val_loss, val_acc = self._calcular_loss(X_val, y_val)
                
                # Restaurar prototipos
                self.prototipos_classes = prototipos_backup
                
                print(f"    Validação - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}")
                
                if val_loss < melhor_loss:
                    melhor_loss = val_loss
                    melhor_modelo = prototipos_locais.copy()
                    contador_patience = 0
                else:
                    contador_patience += 1
                
                if contador_patience >= self.patience:
                    print(f"    Early stopping na época {epoca + 1}")
                    return melhor_modelo
        
        return prototipos_locais
    
    def predict(self, X, usar_dropout=False):
        """Predição melhorada com ensemble"""
        X_norm = self.scaler.transform(X)
        
        if self.ensemble_size == 1:
            return self._predict_individual(X_norm, self.prototipos_classes, usar_dropout)
        else:
            # Votação por ensemble
            todas_predicoes = []
            for modelo in self.ensemble_models:
                pred = self._predict_individual(X_norm, modelo, usar_dropout)
                todas_predicoes.append(pred)
            
            # Votação majoritária
            predicoes_finais = []
            for i in range(len(X)):
                votos = [pred[i] for pred in todas_predicoes]
                predicao_final = max(set(votos), key=votos.count)
                predicoes_finais.append(predicao_final)
            
            return predicoes_finais
    
    def _predict_individual(self, X, prototipos, usar_dropout=False):
        """Predição para um modelo individual"""
        predicoes = []
        
        for exemplo in X:
            vetor = self._codificar_exemplo_melhorado(exemplo, usar_dropout=usar_dropout)
            
            melhor_classe = None
            maior_sim = -np.inf
            
            for classe, prototipo in prototipos.items():
                sim = self.assemble.similaridade(vetor, prototipo)
                if sim > maior_sim:
                    maior_sim = sim
                    melhor_classe = classe
            
            predicoes.append(melhor_classe)
        
        return predicoes
    
    def plot_training_history(self):
        """Plota histórico de treinamento"""
        if self.historico_loss:
            plt.figure(figsize=(12, 4))
            
            plt.subplot(1, 2, 1)
            plt.plot(self.historico_loss, label='Training Loss')
            if self.historico_val_loss:
                plt.plot(self.historico_val_loss, label='Validation Loss')
            plt.xlabel('Época')
            plt.ylabel('Loss')
            plt.title('Histórico de Loss')
            plt.legend()
            
            plt.subplot(1, 2, 2)
            plt.plot([1-l for l in self.historico_loss], label='Training Accuracy')
            if self.historico_val_loss:
                plt.plot([1-l for l in self.historico_val_loss], label='Validation Accuracy')
            plt.xlabel('Época')
            plt.ylabel('Accuracy')
            plt.title('Histórico de Acurácia')
            plt.legend()
            
            plt.tight_layout()
            plt.show()

# Exemplo de uso
if __name__ == "__main__":
    # Gerar dados de exemplo
    from sklearn.datasets import make_classification
    
    X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, 
                             n_redundant=2, n_informative=18, random_state=42)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Modelo melhorado
    modelo = HDCClassificadorMelhorado(
        d_dimensao=15000,
        n_niveis=15,
        modo='hierarchical',
        epocas=10,
        taxa_aprendizado=0.05,
        regularizacao=0.01,
        dropout_rate=0.15,
        ensemble_size=3,
        adaptive_learning=True,
        early_stopping=True,
        patience=3,
        validation_split=0.2
    )
    
    # Treinamento
    modelo.train(X_train, y_train)
    
    # Predição
    predicoes = modelo.predict(X_test)
    
    # Avaliação
    accuracy = accuracy_score(y_test, predicoes)
    print(f"Acurácia final: {accuracy:.4f}")
    
    # Plotar histórico se disponível
    modelo.plot_training_history()